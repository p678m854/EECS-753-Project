\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% Default IEEE packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Additional packages
\usepackage{multirow}
\usepackage{tikz}
\usepackage{epstopdf}

\newcommand{\xs}{-1}
\newcommand{\ys}{0.5}

% Tikz commands
\newcommand{\cube}[6]{  % 2 x-y, 3 dimensions, 1 color
        \begin{scope}[
        	shift={(#1, #2)},
			yslant=\ys ,xslant=\xs
            ]
            \filldraw[fill=#6] (0,0) rectangle (#3, #4);
            \draw[black] (0,0) rectangle (#3,#4);%marking borders
        \end{scope}
        \begin{scope}[
        	shift={(#1, #2)},
            yslant=\ys ,xslant=0
            ]
            \filldraw[fill=#6] (0,0) rectangle (#3, -#5);
            \draw[black] (0,0) rectangle (#3,-#5);%marking borders
        \end{scope}
        \begin{scope}[
        	shift={(#1, #2)},
            yslant=-\ys ,xslant=0
            ]
            \filldraw[fill=#6] (0,0) rectangle (-#4, -#5);
            \draw[black] (0,0) rectangle (-#4,-#5);%marking borders
        \end{scope}
}

\newcommand{\gridcube}[6]{  % 2 x-y, 3 dimensions, 1 color
        \begin{scope}[
        	shift={(#1, #2)},
            every node/.append style={
            yslant=0.5,xslant=-1},yslant=0.5,xslant=-1
            ]
            \filldraw[fill=#6] (0,0) rectangle (#3, #4);
            \draw[step=1, black!50] (0,0) grid (#3,#4); %defining grids
            \draw[black,very thick] (0,0) rectangle (#3,#4);%marking borders
        \end{scope}
        \begin{scope}[
        	shift={(#1, #2)},
            every node/.append style={
            yslant=0,xslant=0},yslant=0.5,xslant=0
            ]
            \filldraw[fill=#6] (0,0) rectangle (#3, -#5);
            \draw[step=1, black!50] (0,0) grid (#3,-#5); %defining grids
            \draw[black,very thick] (0,0) rectangle (#3,-#5);%marking borders
        \end{scope}
        \begin{scope}[
        	shift={(#1, #2)},
            every node/.append style={
            yslant=0,xslant=0},yslant=-0.5,xslant=0
            ]
            \filldraw[fill=#6] (0,0) rectangle (-#4, -#5);
            \draw[step=1, black!50] (0,0) grid (-#4,-#5); %defining grids
            \draw[black,very thick] (0,0) rectangle (-#4,-#5);%marking borders
        \end{scope}
}

\newcommand{\tikzcircle}[2][red,fill=red]{\tikz[baseline=-0.5ex]\draw[#1,radius=#2] (0,0) circle ;}%

    
\begin{document}

\title{Curvy DeepPicar
\thanks{Accompanying GitHub Repo: https://github.com/p678m854/EECS-753-Project}
}

\author{\IEEEauthorblockN{Patrick McNamee}
\IEEEauthorblockA{\textit{Department of Electrical Engineering and Computer Science} \\
\textit{University of Kansas}\\
Lawrence, KS \\
p678m854@ku.edu}}

\maketitle

\begin{abstract}

Convolutional Neural Networks (CNNs) are a common implementation for vision based driving on autonomous vehicles. One common technical issue for these CNN vision based systems is that the neural network outputs tend to be very noise with no guarantees of output smoothness as the vehicle traverses the environment. Previous work has shown that using the network outputs as B\`ezier curves for an outer loop navigation controller with an inner pursuit curve controller leads to smooth performances and outperform other network architectures. This work follows a natural extension implements a CNN with B\`ezier curve outputs as part of the controller module rather navigational module on a physical platform for vision navigation and evaluates the results against a standard CNN as a direct controller. Both evaluation of final training results and a physical experiment indicate that implementing B\`ezier CNNs as either direct controllers or an outer loop navigation controller would improve the performance of autonomous automobiles for lane navigation.

\end{abstract}

\begin{IEEEkeywords}
machine learning, autonomous vehicles, embedded platforms, guidance navigation and control, convolutional neural networks
\end{IEEEkeywords}

\section{Introduction}

Modern car manufacturers are developing autonomous automobiles for future use in both civilian and military applications. There are various complex environmental aspects that are required for consideration in order for autonomous agents to succeed in driving tasks such as sensors for object detection and localization, path planning, and state prediction for object avoidance \cite{kato2018}. These aspects traditionally rely on computationally complex and resource intensive algorithms that may be too slow to implement on a real-time system. An alternative is to use neural networks which are universal approximators \cite{wang2021interval} to approximate the algorithms for a loss of functional accuracy but an increase in computational throughput. As such, neural networks have been frequently used for autonomous vehicle.

Usage of neural networks in autonomous ground vehicles has been around for decades with ALVINN being one of the first examples of vision-based navigation \cite{pomerieau-1989}. ALVINN used a 32 by 32 video input along with a 8 by 32 range finder input feed into a hidden layer of 29 units before the output layer of 46 units. The wheel steering angle was chosen based on the values of the 46 output units and ALVINN was quite successful at its various driving tasks. Modern implementations of autonomous cars, like the one developed by NVIDIA, are empowered by modern computational performance have since moved from the relatively simple network architecture used in ALVINN and currently use CNN which includes convolutional layers as well as significantly larger layers size \cite{bojarski2016end}. While these larger neural networks have allowed for more complex behavior of the autonomous agent, there are still issues that result from the nature of neural networks. Often the outputs of neural networks are rough in the sense  small manipulations on the input layer can produce drastically different outputs and there are no guarantees of output behavior in unexplored environments. Still, CNNs are potential methods to work towards fully autonomous driving and there are several implementation techniques to include them.

\subsection{Background}

There are various ways for CNNs to be implemented for autonomous vision-based automobile navigation as reference in Fig. \ref{fig:vb-steering}. The most direct method is to have the CNN take images and translate them into direct control inputs such as the steering angle or throttle as in \cite{bechtel2018}. While direct, this implementation is limited in that the controller is limited by the input rate of the camera systems as well as the network throughput implementation. It is also susceptible to network output roughness which can lead to high control jerk and unnecessarily fatigued physical components. This implementation of a direct controller is referred to in the rest of this work as image-to-point for translating a camera image into a point in the control input space.

Two alternative implementations that are indirect schemes are navigational implementations demonstrated in DeepRacing \cite{trent2020iros}. Rather than directly control the car, the CNN outputs desired positions for navigate with a lower level control scheme closing the loop. There are two different ways to represent navigational points using discrete and continuous methods. A discrete method is simply to transform the input into a collection of ordered waypoints for navigation. However, a CNN with rough outputs will still produce noisy outputs that may cause unwanted behavior. Additionally as the number of waypoints changes, the network architecture needs to be adjusted.

Rather than represent the waypoints or as discrete points, the waypoints can be represented as a continuous polynomial curves. B\`ezier curves $\mathbf{B}$ are useful representation as any polynomial of degree $d$ can be implemented with $d+1$ points $\mathbf{P}_k$ in $k$ dimensional space as poles by \eqref{eq:bezier-curve} using a parameter $t$.

\begin{equation}
\label{eq:bezier-curve}
\mathbf{B}(t) = \sum_{k=0}^{d} \genfrac(){0pt}{0}{d}{k} (1-t)^{d-k}t^k \mathbf{P}_k \quad t\in [0, 1]
\end{equation}

This representation is advantageous as it can represent the same time window as the discrete waypoint representation but has an infinite number of points so the number of waypoints generated is not upper bounded. Additionally the waypoints are guaranteed to be smooth since they are on a polynomial curve and experimentally DeepRacing showed that this B\`ezier representation was the best performing network for CNNs in the simulated racing domain \cite{trent2020iros}. While previous results are from simulations, to the authors knowledge this has not been verified any physical platform.

\begin{figure}[hbtp]
	\centerline{
	\begin{tikzpicture}
		% labeling
		\node at (-1.5,4) {a)};
		\node at (-1.5,2) {b)};
		\node at (-1.5,0) {c)};
		
		% Plot the basic images
		\foreach \y in {0,2,4}{
			\node at (0,\y) {\includegraphics[width=1.5cm,height=1cm]{..//figures/presentation/out-video-1-moment.jpg}};  % Image from camera
			\draw[blue, ultra thick, -latex] (0.8,\y) -- (1.45,\y);
			\filldraw[black, fill=black!60] (1.5,\y-0.25) rectangle node[white] {CNN} (2.5,\y+0.25);  % Neural Network
			\draw[red, ultra thick, -latex] (2.6,\y) -- (3.1,\y);
		}
		% Image-to-Point
		\draw (3.2,4) node[anchor=west] {$\theta_{\text{steering}} = 30^\circ$};
		% Image-to-Waypoints
		\draw[black, latex-latex] (3.2,1.5) -- (3.2, 2.5) node[anchor=south]{$y$};
		\draw[black, -latex] (3.2,2) -- (5,2) node[anchor=west]{$x$};
		\foreach \dx in {0.1,0.3,...,1.5}
			{\filldraw[red, fill=red] ( 3.2 + \dx , {2 - 0.15*pow(\dx, 2)} ) circle (1.5pt);}
		% Image-to-Curve
		\draw[black, latex-latex] (3.2,-0.5) -- (3.2, 0.5) node[anchor=south]{$\theta$};
		\draw[black, -latex] (3.2,0) -- (5,0) node[anchor=west]{$t$};
		\draw[red] (3.2,0) .. controls (3.75,0.5) and (4.25,-0.5) .. (4.9,-0.2);
		\draw[red] (3.2,0) circle (1.5pt);
		\draw[red] (3.75,0.5) circle (1.5pt);
		\draw[red] (4.25,-0.5) circle (1.5pt);
		\draw[red] (4.9,-0.2) circle (1.5pt);
		
	\end{tikzpicture}
	}
	\caption{Various Vision-Based Autonomous Steering Implementations. From top to bottom; a) image to direct control command, b) image to localized waypoints (\tikzcircle{2pt}), and c) image to curve using poles ({\color{red} $\circ$}).}
	\label{fig:vb-steering}
\end{figure}

While most work is focused on personal or racing automobiles, there have been implementations of vision-based navigation on small, radio-controlled cars. These vehicles are significantly cost-effective as research platforms and testing embedded systems. Previous work at the University of Kansas has demonstrated implementing the NIVIDIA DAVE-2 neural network architecture on a Raspberry Pi for a CNN image-to-point vision navigation system  on a physical platform referred to as DeepPicar \cite{bechtel2018}. From a cost standpoint, the DeepPicar platform is ideal for testing a real implementation of a CNN with B\`ezier curve output although due to hardware and sensor limitations, an image-to-curve controller with the curve in the control input space will be used to compare the B\`ezier implementation of a controller against a standard image-to-point CNN controller rather than having the B\`ezier CNN be the navigational controller as in \cite{trent2020iros}. This is to avoid any heavy computation loads due to simultaneous localization and mapping (SLAM) algorithms which are required for positional estimates on DeepPicar.

\subsection{Motivations}

Previous work has shown the use of CNNs outputting B\`ezier curves for navigation problems in simulated racing outperforms other techniques such as a CNN outputting navigation waypoints or direct control networks \cite{trent2020iros}. However the simulation relies on position estimates which may not be feasible onboard small platforms or in environments with no direct position estimates. Additionally, the B\`ezier CNNs have, to the knowledge of the author, not been tested on a physical platform as of this work. Hence this work seeks to implement the a CNN outputting B\`ezier curves in a direct control scheme for navigating a track on DeepPicar to validate findings from DeepRacing and extend the usage of B\`ezier curves to solve control problems in addition to previous investigated navigation problems.

\section{Novel Contributions}

This work will implement a CNN outputting B\`ezier control curves for an embedded autonomous automobile platform for course navigation which has not been tested before on a physical platform or as a direct control command. This new implementation will be tested against a standard CNN with direct control output as used in previous work \cite{bechtel2018} on the same physical platform.

\section{Convolutional Neural Networks}

\begin{figure*}[bt]
	\centerline{
		\begin{tikzpicture}[yscale=0.01, xscale=0.02]
		\cube{0}{0}{200}{3}{66}{yellow}
		\cube{40}{-10}{200}{3}{66}{blue!50}
		\cube{140}{-5}{98}{24}{31}{red!10}
		\cube{220}{-20}{47}{36}{14}{red!20}
		\cube{290}{-45}{22}{48}{5}{red!40}
		\cube{360}{-80}{20}{64}{3}{red!80}
		\cube{430}{-115}{18}{64}{1}{red}
		\cube{400}{-140}{100}{8}{8}{green!10}
		\cube{435}{-135}{50}{8}{8}{green!20}
		\cube{465}{-130}{10}{8}{8}{green!40}
		\cube{478}{-134}{1}{8}{8}{green!80}
		\end{tikzpicture}
	}
	\caption{Visualization of DAVE-2 Neural Network \cite{bojarski2016end}. Yellow is an input image, blue is the regularization layer, red is a convolution layer, and green is a dense layer. Dense layers are exaggerated for viewing and flattening omitted.}
	\label{fig:dave-2}
\end{figure*}

The CNNs are based off of the DAVE-2 CNN architecture from NIVIDIA \cite{bojarski2016end} and is shown in Fig. \ref{fig:dave-2}. Previous work has been implemented on a Raspberry Pi 3B+ for the DeepPicar \cite{bechtel2018} and this work will use the same implementation as \cite{bechtel2018}, albeit on a Raspberry Pi 3A+, to form a direct comparison with previous implementations. The input to the CNNs is a Playstation Eye Camera which generates $320\times 240$ RGB image frames which are resized to $200\times 66$ to match the DAVE-2 inputs. Inside the CNNs, the image is normalized with default Tensorflow 2.x layer parameter values across the image width, height, and channels before passing though multiple convolution layers. Layers 2, 3, and 4 use $5\times 5$ kernels with a width and height stride of $(2,2)$ while layers 5 and 6 use $3\times 3$ kernels with a width and height stride of $(1,1)$. After layer 6, the network is flattened to dense layers whose layer width is reduced until a single output unit. The activation functions used for layers 2 through 10 are rectilinear linear units (ReLU) while the last layer uses a hyperbolic tangent function. These activation functions are consistent with implementations in previous work \cite{bechtel2018,bojarski2016end} but the use of ReLU for the hidden layers are consistent with proofs that neural networks are universal approximators \cite{HORNIK1989359} and the hyperbolic tangent allows for outputs to be bounded to match the automobiles steering wheel angle limits $ [\theta_{\min},\ \theta_{\max}] $ by a linear transformation.

\subsection{Training Datasets}


There are two publicly datasets available, and investigated, to train radio-control autonomous cars for tracks. The first dataset is a popular, non-academic, but relatively small and non-timestamped dataset consisting of only 219 example images but has a continuous steering angle ranging \cite{tian2019} while the other dataset is the DeepPicar dataset which consists of 11,000 timestamped example images but only has a collection of discrete wheel angles in the set $\lbrace -30^\circ,\ 0^\circ,\ 30^\circ \rbrace$ degrees but uses the same physical platform as this work \cite{bechtel2018}. As the B\`ezier curves deal with continuous outputs, it would be more advantageous from a training perspective to attempt to use Data Augmentation to train the various CNN models on the first dataset and then evaluate the CNN models on the larger second dataset to see if any trained model can be transferred to the matching physical platform. For the training on \cite{tian2019}, all images were loaded into memory and an application of a horizontal image flight was applied to augment the dataset. The target steering wheel angles were scaled so that the models was only attempting to output in the range $[-1,\ 1]$ where $-1$ and $1$ indicate a steering wheel angle of $-30^\circ$ degrees and $30^\circ$ i.e. $30^\circ$ left and right respectively. For train the CNN whose architecture is in Table \ref{tab:i2c-arch}, a 80-20 test-validation set split was used with a mean squared error (MSE) loss function and the model was trained for 100 epochs using the ADAM optimizer with parameters in Table \ref{tab:training-parameters}.

\begin{figure}[tbhp]
	\centerline{\includegraphics[width=1.5in]{"../../data/external/DeepPicar-data/video01_000_085.png"}\hspace*{0.1in}\includegraphics[width=1.5in]{../figures/presentation/out-video-1-moment.jpg}}
	\caption{Examples from the datasets where left is from \cite{tian2019} and right is from \cite{bechtel2018}.}
\end{figure}

\begin{table}[btp]
        \centering
            \caption{Image-to-Point CNN Model using Domain Augmentation (Acc: 38.71\%)}
            \begin{tabular}{|c|r|c|c|c|}
            \multicolumn{2}{c}{} & \multicolumn{3}{c}{\bfseries Model Predicted}\\\cline{3-5}
            \multicolumn{1}{c}{} & & \textbf{\textit{Left}} & \textbf{\textit{Center}} & \textbf{\textit{Right}}\\\hline
             \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\bfseries Record}}} & \textbf{\textit{Left}} & 141 & 1,147 & 28 \\\cline{3-5}
            & \textbf{\textit{Center}} & 820 & 3,334 & 306 \\\cline{3-5}
            & \textbf{\textit{Right}} & 850 & 3,591 & 783 \\\cline{3-5}\hline
            \end{tabular}
        \label{tab:i2p-da-acc}
    \end{table}

To analyze the performance of the model in the new domain of \cite{bechtel2018} which has a discrete controller, the CNN model was used to predict the continuous wheel angle for all 1,000 frames of each of the 11 videos and the output was rounded to the closes steering angle in $\lbrace -30^\circ,\ 0^\circ,\ 30^\circ \rbrace$. A cross-correlation matrix of the model outputs versus the record steering wheel angles are displayed in Table \ref{tab:i2p-da-acc}. The accuracy was not deemed high enough to use the model in the domain of \cite{bechtel2018} i.e. the DeepPicar platform so data augmentation cannot be used in this work. However, the \cite{bechtel2018} dataset outputs still need to be  modified to appear to be continuous. To achieve this effect, the CNN models will train on a central moving average filter of the steering wheel angles with a filter window of 5 data points which are nominally spaced at 50 milliseconds.

\subsection{Image-to-Point CNN}

The image-to-point CNN is a recreation of the models trained in previous work \cite{bechtel2018,bojarski2016end} with the various layers types, dimensions, and total parameters listed in Table \ref{tab:i2p-arch}. There is a discrepancy between the original DeepPicar previous work \cite{bechtel2018} and \cite{bojarski2016end} where the first normalization layer is excluded from the CNN. Since the original DeepPicar was using the DAVE-2 CNN as the reference model, this work will follow the DAVE-2 model more strictly and include the layer normalization. For training, the CNN will use the mean squared error (MSE) loss function with the recorded wheel angles being proportionally scaled to the range $[1, -1]$. Afterwards, the model will be implemented onto the DeepPicar as an image-to-point navigation controller operating at 20 Hz.

\begin{table}[tbp]
	\centering
	\caption{Image-to-Point Convolutional Neural Network Architecture}
	\begin{tabular}{|r|c|c|r|}
	\multicolumn{1}{c}{\bfseries Layer} & \multicolumn{1}{c}{\bfseries Layer Type} & \multicolumn{1}{c}{\bfseries Dimension} & \multicolumn{1}{c}{\bfseries Parameters} \\ \hline
	1 & Normalizer & $66 \times 200 \times 3$ & 79,200 \\
	2 & 2D Convolution ($5\times5$)& $31 \times 98 \times 24$ & 1,824 \\
	3 & 2D Convolution ($5\times5$)& $14 \times 47 \times 36$ & 21,636 \\
	4 & 2D Convolution ($5\times5$)& $5 \times 22 \times 48$ & 43,248 \\
	5 & 2D Convolution ($3\times3$)& $3 \times 20 \times 64$ & 43,248 \\
	6 & 2D Convolution ($3\times3$)& $1 \times 18 \times 64$ & 43,248 \\
	7 & Flattening & $1152$ & 0 \\
	8 & Dense & $100$ & 115,300 \\
	9 & Dense & $50$ & 5,050 \\
	10 & Dense & $10$ & 510 \\
	11 & Dense & $1$ & 11 \\ \hline
	\multicolumn{2}{c|}{} & Non-trainable & 79,200 \\
	\multicolumn{2}{c|}{} & Trainable & 252,219 \\ \cline{3-4}
	\multicolumn{2}{c|}{} & Total & 331,419 \\ \cline{3-4}
	\end{tabular}
	\label{tab:i2p-arch}
\end{table}

\subsection{Image-to-Curve CNN}

The image-to-curve CNN cannot be the same network architecture as the image-to-point architecture as the outputs of the CNNs are fundamentally different. Image-to-point controllers have a one-to-one mapping so if the image is being taken from the camera for navigation at a fixed rater, the wheel angle will also be scheduled to update at the same fixed rate, in this work 20 Hz. However, the B\`ezier CNN operates with a time window for the paramter $t$ to be in the range $[0,\ 1]$ so the mapping can be one-to-many i.e. the B\`ezier CNN neural network operating at a slower rate than the wheel angle controller and trains to fit the B\`ezier curve to all points in the time window. For this work, time window selected for the $t$ parameter was 0.5 seconds but it is possible for the time windows to overlap. The B\`ezier CNN will always predict for the full time window but the overall navigation could update the pole point values say every 0.25 seconds. If there is no time window overlap than the image-to-curve CNN operates as an outer loop controller at 2 Hz with the a wheel angle controller updating at 20 Hz which means there is 10:1 ratio between wheel angles and camera frames assuming both controllers are real-time tasks. This reduction in required CNN task rate reduces the computational load for the embedded system as the CNN frame processing occurs less frequently to the image-to-point models but this reduction depends on the amount overlap between subsequent time windows. If there is some overlap then the 10:1 ratio can be reduced up to a 1:1 ratio as with the image-to-point CNN i.e. the B\`ezier CNN is being used to generate new poles at a 20 Hz rate. In the extreme 1:1 case, the B\`ezier CNN can simply use the first, i.e. 0 indexed pole, to use as the steering wheel angle in an image-to-point control scheme which may yield different results than just the standard image-to-point CNN controller as the B\`ezier CNN trains with future steering angles. Two overlap ratios will be tested, a 10:1 and 1:1 ratio to see the overall behavior.

\begin{table}[tbp]
	\centering
	\caption{B\`ezier Convolutional Neural Network Architecture}
	\begin{tabular}{|r|c|c|r|}
	\multicolumn{1}{c}{\bfseries Layer} & \multicolumn{1}{c}{\bfseries Layer Type} & \multicolumn{1}{c}{\bfseries Dimension} & \multicolumn{1}{c}{\bfseries Parameters} \\ \hline
	1 & Normalizer & $66 \times 200 \times 3$ & 79,200 \\
	2 & 2D Convolution ($5\times5$)& $31 \times 98 \times 24$ & 1,824 \\
	3 & 2D Convolution ($5\times5$)& $14 \times 47 \times 36$ & 21,636 \\
	4 & 2D Convolution ($5\times5$)& $5 \times 22 \times 48$ & 43,248 \\
	5 & 2D Convolution ($3\times3$)& $3 \times 20 \times 64$ & 43,248 \\
	6 & 2D Convolution ($3\times3$)& $1 \times 18 \times 64$ & 43,248 \\
	7 & Flattening & $1152$ & 0 \\
	8 & Dense & $100$ & 115,300 \\
	9 & Dense & $50$ & 5,050 \\
	10 & Dense & $4$ & 204 \\
	11 & Reshape & $4\times 1$ & 0 \\ \hline
	\multicolumn{2}{c|}{} & Non-trainable & 79,200 \\
	\multicolumn{2}{c|}{} & Trainable & 251,902 \\ \cline{3-4}
	\multicolumn{2}{c|}{} & Total & 331,102 \\ \cline{3-4}
	\end{tabular}
	\label{tab:i2c-arch}
\end{table}

\begin{figure}
	\centerline{
		\begin{tikzpicture}[scale=0.175]
		\node at (0,7) {Model Prediction ($y_{\text{p}}$)};
        % Top
        \begin{scope}[
            yshift=-83,every node/.append style={
            yslant=0.5,xslant=-1},yslant=0.5,xslant=-1
            ]
            \filldraw[fill=blue!20] (0,0) rectangle (4, 1);
            \draw[step=1, black] (0,0) grid (4,1); %defining grids
            \draw[black,very thick] (0,0) rectangle (4,1);%marking borders
        \end{scope}
        \begin{scope}[
            yshift=-83,every node/.append style={
            yslant=0,xslant=0},yslant=0.5,xslant=0
            ]
            \filldraw[fill=red!20] (0,0) rectangle (4, -10);
            \draw[step=1, black] (0,0) grid (4,-10); %defining grids
            \draw[black,very thick] (0,0) rectangle (4,-10);%marking borders
        \end{scope}
        \begin{scope}[
            yshift=-83,every node/.append style={
            yslant=0,xslant=0},yslant=-0.5,xslant=0
            ]
            \filldraw[fill=green!20] (0,0) rectangle (-1, -10);
            \draw[step=1, black] (0,0) grid (-1,-10); %defining grids
            \draw[black,very thick] (0,0) rectangle (-1,-10);%marking borders
            \node[align=center,rotate=90,anchor=south] at (-1,-5) {Example\\ Index};
        \end{scope}
        
        \draw (-0.5,1.0) node[rotate=27,align=center] {Pole\\ Index};
        \node at (5.5,0) {$\mathbf{P}$};
        \end{tikzpicture}
        \hspace{0.25in}
        \begin{tikzpicture}[scale=0.165]
        \node at (3,8) {Target Outputs ($y_{\text{t}}$)};
        % Top
        \begin{scope}[
            yshift=-83,every node/.append style={
            yslant=0.5,xslant=-1},yslant=0.5,xslant=-1
            ]
            \filldraw[fill=blue!20] (0,0) rectangle (10, 1);
            \filldraw[fill=yellow!20] (0,1) rectangle (10, 2);
            \draw[step=1, black] (0,0) grid (10,2); %defining grids
            \draw[black,very thick] (0,0) rectangle (10,2);%marking borders
        \end{scope}
        \begin{scope}[
            yshift=-83,every node/.append style={
            yslant=0,xslant=0},yslant=0.5,xslant=0
            ]
            \filldraw[fill=red!20] (0,0) rectangle (10, -10);
            \draw[step=1, black] (0,0) grid (10,-10); %defining grids
            \draw[black,very thick] (0,0) rectangle (10,-10);%marking borders
            \node[xshift=4pt,yshift=5pt] at (10,0) {$\mathbf{P}$};
            \node[xshift=0pt,yshift=11pt] at (9,0) {$t$};
        \end{scope}
        \begin{scope}[
            yshift=-83,every node/.append style={
            yslant=0,xslant=0},yslant=-0.5,xslant=0
            ]
            \filldraw[fill=green!20] (0,0) rectangle (-2, -10);
            \draw[step=1, black] (0,0) grid (-2,-10); %defining grids
            \draw[black,very thick] (0,0) rectangle (-2,-10);%marking borders
            \node[align=center, rotate=90,anchor=south] at (-2,-5) {Example\\ Index};
        \end{scope}
        
        \draw (1,2) node[rotate=27] {Sample Index};
        \end{tikzpicture}
	}
	\caption{Model Prediction and Target Outputs}
	\label{fig:bl-in-out}
\end{figure}


Training the B\`ezier CNN is different from standard neural networks as the model predictions and the target outputs for training are differently shaped tensors as shown in Fig. \ref{fig:bl-in-out}. Where the image-to-point CNN was only responsible for the steering wheel angle $\theta_{\text{steering}} \in \mathbb{R}^1$, the image-to-curve CNN outputs poles $\mathbf{P}_k$ to form the B\`ezier curve hence the CNN output needs to exist in $\mathbb{R}^{(d+1)\times 1}$ space. To accomplish this, the last two layers are changed from the image-to-point CNN to the image-to-curve CNN with the polynomial output being a cubic with the resulting network architecture being shown in Table \ref{tab:i2c-arch}. For the target outputs during the training session, both the point and associated non-dimensional parameter $t$ value must be recorded. This leads leads to a mismatch in dimension size where, assuming the first tensor dimension is the example index, the second dimension of the tensors do not match. The model output has the second dimension reserved for indexing the poles forming the curve while the target output uses the second dimension to index the sample recorded points to fit the curve to.

Previous work rectified this dimensional discrepancy by generated output points $\mathbf{B}$ using a matrix representation $\mathbf{B} = \mathbf{A}(\mathbf{t},d) \mathbf{P}$ where $\mathbf{A}(\mathbf{t},d)$ is the matrix whose elements are $(1-t)^{d-k}t^k \genfrac(){0pt}{0}{d}{k}$ from \eqref{eq:bezier-curve} determining points by the matrix of output poles $\mathbf{P}$ and the corresponding parameter vector $\mathbf{t}$ \cite{trent2020iros}. While this implementation is simple, readily converted to a graph structure for GPU training, and easily integratable with preexisting loss functions it overly constrains the outputs as $\mathbf{A}$ needs to be determined a priori so the sampling can only happen at predetermined points. This work instead uses a custom implementation with functional mapping in Tensorflow 2.x to implement \eqref{eq:bl} which is sum of squared errors for arbitrary $n$ dimensional point spaces. As the implementation is a functional mapping, it is a more flexible approach that is robust to unique time window sampling spaces as well as ragged tensors i.e. the number of samples in a time window could potentially be different. A visual comparison of the model predicted tensors and the target outputs for applying \eqref{eq:bl} is shown in Fig. \ref{fig:bl-in-out} and the recorded steering wheel angle values have been been mapped to $[-1,\ 1]$ such that the extreme angles are $\pm 45^\circ$ degree turns so that the poles can allow for an intermediate point in the time window to achieve the vehicle limit of $\pm 30^\circ$ degree turns starting from a non-extreme value.

\begin{equation}
\label{eq:bl}
\begin{split}
\mathcal{L}_\mathbf{B}&(y_{\text{p}},\ y_{\text{t}}) = \sum_{e} \sum_{s} \left\| y_{\text{t}}(e,s,1:) -  \right. \\
&\left. \sum_{k=0}^d (1 - y_{\text{t}}(e,s,0))^{d-k} y_{\text{t}}(e,s,0)^k y_{\text{p}}(e,k,:) \right\|_2^2\\
\end{split}
\end{equation}

\subsection{Training}

The dataset is randomly split into a train set of 9 videos and a test set of 2 videos. Each of the videos have output keys generated in previous work where each video frame is matched to a recorded steering wheel angle. Since loading in all of the videos at once is resource expensive as the available hardware for this work only has roughly 2 Gigabytes of RAM available for the holding the input and output tensors, an unusual training scheme is performed. Rather than training over all frames of the 9 videos in batches as is traditional, the models are trained on a video as specified by sequentially iterating over an ordered list of videos before looping back and training on the list again. For training on a video, 200 frames are loaded into memory with a horizontal flip augmentation to yield 400 training examples for the model to train on. After training, the next 200 frames are loaded in training and this repeats until the model has trained on all 1,000 frames of a video. The specific training parameters and listings are shown in Table \ref{tab:training-parameters} and the training histories are shown in Fig. \ref{fig:th}. The training history shows periodic behavior which generally does not appear in traditional training schemes but overall loss function seems to be reducing as the training persists with minimal reductions towards the end of training. This indicates that the CNN models have not been overfitted and may proceed to be evaluated.

\begin{table}[btp]
	\centering
	\caption{Training parameters}
	\begin{tabular}{|r|c|}
	\multicolumn{1}{c}{\textbf{Parameter}} & \multicolumn{1}{c}{\textbf{Value}} \\\hline
	Training Videos & [2, 9, 7, 8, 10, 1, 11, 6, 4] \\
	Loops over Training Set & 4\\
	Test Videos & 3 and 5 \\
	Epochs/200 Frames & 50 \\
	Batch Size & 256 \\
	Optimizer & ADAM \\
	Learning Rate & $\alpha=0.001$ \\
	Decay Rate & $\beta_1 = 0.9$, $\beta_2=0.999$ \\ \hline
	\end{tabular}
	\label{tab:training-parameters}
\end{table}

\begin{figure}[bt]
	\centerline{\includegraphics[width=3.5in]{../figures/final-report/cnn-i2p-th}}
	\centerline{\includegraphics{../figures/final-report/cnn-i2c-th}}
	\caption{Training history of standard CNN (top) and B\`ezier CNN (bottom).}
	\label{fig:th}
\end{figure}

\section{Experiments}

There are two methods to analyze the quality of the CNN; a quantitative accuracy check using the test videos and a qualitative check using a navigational track. These two methods are consistent with previous work \cite{bechtel2018} and allows for a fair comparison of the models in this work.

\subsection{Model Accuracy}

Accuracy to the record values of the steering wheel is used to quantitatively evaluate the model quality for the various approaches. Accuracy is determined by iterating through all 2,000 total frames of the Video 3 and 5 of the test set in \cite{bechtel2018} and comparing continuous CNN outputs to their closest correspondence in the discrete $\lbrace -30^\circ,\ 0^\circ,\ 30^\circ\rbrace$ degree viable wheel angles with results shown in Tables \ref{tab:i2p-acc}, \ref{tab:i2bc-acc}, and \ref{tab:i2bp-acc}. The comparison previous work \cite{bechtel2018} using the same CNN architecture as the image-to-point network has shown accuracy of approximately 70\% although those models do not have normalization layer which is thought to be the reason why the baseline accuracy CNN are so poor in comparison. However, the results do show that using B\`ezier curves in the model networks do drastically improve the performance as they match the truth values at higher accuracy. Interestingly, reducing the ratio of control points to image frames down to a 1:1 drastically improved the CNN controller which indicates that using future steering angles greatly helps in improving the one-off prediction accuracy of image-to-point CNN controllers. Videos involving the overlay of the predictions are in the accompanying GitHub repository for validation in the video quality check folder of the reports subdirectory.


\begin{table}[btp]
	\centering
	\caption{Image-to-Point CNN Model (Acc: 20.45\%)}
	\begin{tabular}{|c|r|c|c|c|}
       \multicolumn{2}{c}{} & \multicolumn{3}{c}{\bfseries Model Predicted}\\\cline{3-5}
       \multicolumn{1}{c}{} & & \textbf{\textit{Left}} & \textbf{\textit{Center}} & \textbf{\textit{Right}}\\\hline
        \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\bfseries Record}}} & \textbf{\textit{Left}} & 48 & 31 & 21 \\\cline{3-5}
        & \textbf{\textit{Center}} & 156 & 156 & 188 \\\cline{3-5}
		& \textbf{\textit{Right}} & 54 & 136 & 205 \\\cline{3-5}\hline
	\end{tabular}
	\label{tab:i2p-acc}
\end{table}
    
\begin{table}[btp]
	\centering
	\caption{Image-to-Curve B\`ezier Model (Acc: 38.10\%) }
	\begin{tabular}{|c|r|c|c|c|}
		\multicolumn{2}{c}{} & \multicolumn{3}{c}{\bfseries Model Predicted}\\\cline{3-5}
		\multicolumn{1}{c}{} & & \textbf{\textit{Left}} & \textbf{\textit{Center}} & \textbf{\textit{Right}}\\\hline
		\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\bfseries Record}}} & \textbf{\textit{Left}} & 83 & 46 & 33 \\\cline{3-5}
		& \textbf{\textit{Center}} & 268 & 343 & 313 \\\cline{3-5}
		& \textbf{\textit{Right}} & 177 & 268 & 336 \\\cline{3-5}\hline
	\end{tabular}
	\label{tab:i2bc-acc}
\end{table}

\begin{table}[btp]
	\centering
	\caption{Image-to-Point B\`ezier Model (Acc: 66.75\%)}
	\begin{tabular}{|c|r|c|c|c|}
		\multicolumn{2}{c}{} & \multicolumn{3}{c}{\bfseries Model Predicted}\\\cline{3-5}
		\multicolumn{1}{c}{} & & \textbf{\textit{Left}} & \textbf{\textit{Center}} & \textbf{\textit{Right}}\\\hline
		\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\bfseries Record}}} & \textbf{\textit{Left}} & 60 & 111 & 9 \\\cline{3-5}
		& \textbf{\textit{Center}} & 48 & 667 & 261 \\\cline{3-5}
		& \textbf{\textit{Right}} & 13 & 223 & 608 \\\cline{3-5}\hline
	\end{tabular}
	\label{tab:i2bp-acc}
\end{table}

\subsection{Physical Driving}

While the models were trained int Tensorflow 2.x, the DeepPicar uses a Raspberry Pi specifically a Raspberry Pi 3A+ in this work. This platform does resource limitations and to the best effort of this work cannot install any Tensorflow modules after Tensorflow 1.11 and anyone trying to replicate this work needs to be wary of the size of various Python modules being installed. There are known issues with using Python 3.7 and onwards with Tensorflow 1.x on the Raspberry Pis and some dependencies modules from Tensorflow will cause issues during installation. The associated GitHub Repo of this work will got into more details on the software setup for the DeepPicar. There is a slight modification of implementation of the CNNs for backwards compatibility to Tensorflow 1.11. The first layer of all the CNNs trained in this work was changed from a normalization layer to a batch normalization layer which for batch sizes of one image during deployment are nearly identical. The only difference is that the batch normalization layer used a moving average of the mean and standard deviations during inferences but considering the time frame of the experiments will be roughly 50 seconds, this was deemed acceptable as it was still a reasonably close layer behavior match.

\begin{figure*}[tbp]
	\centerline{
		\begin{tikzpicture}[scale=0.15, line width=.504cm]
		% Blue outline
		\draw[blue!20] (0,0) -- ++(30.5,0) -- ++({6.86*cos(45)} ,{6.86*sin(45)} ) -- ++(0,5.84) -- ++ ({5.04*cos(45)}, {5.04*sin(45)}) -- ++ (28.7,0) -- ++ ({4.57*cos(45)}, {-4.57*sin(45)}) -- ++ (0, -8.64) -- ++ ({14.2*cos(45)}, {-14.2*sin(45)}) -- ++ (19.3,0);
		% Centerlines
		\draw[dashed, very thick] (0,0) --node[anchor=south, yshift=5pt]{305 cm} ++(30.5,0) -- node[anchor=east, yshift=6pt, xshift=-4pt]{68.6 cm} ++({6.86*cos(45)} ,{6.86*sin(45)} ) -- node[anchor=west, xshift=8pt]{58.4 cm} ++(0,5.84) -- node[anchor=east, yshift=6pt, xshift=-4pt]{50.4 cm} ++ ({5.04*cos(45)}, {5.04*sin(45)}) -- node[anchor=south, yshift=5pt] {287 cm} ++ (28.7,0) -- node[anchor=west, yshift=6pt, xshift=4pt] {45.7 cm} ++ ({4.57*cos(45)}, {-4.57*sin(45)}) -- node[anchor=west, xshift=10pt] {86.4 cm} ++ (0, -8.64) -- node[anchor=west, yshift=6pt, xshift=4pt] {142 cm} ++ ({14.2*cos(45)}, {-14.2*sin(45)}) -- node[anchor=south, yshift=5pt] {193 cm} ++ (19.3,0);
		% Start
		\draw[thin] (-6,-1.5) rectangle (0,1.5);
		\foreach \x in {-6,-4.5,...,-1.5}
		{
			\foreach \y in {-1.5,0}
				{
					\filldraw[black, fill=black, thin] (\x,\y) rectangle (\x+0.75,\y+0.75);
					\filldraw[black, fill=black, thin] (\x+0.75,\y+0.75) rectangle (\x+1.5,\y+1.5);
				}
		}
		\draw (-3,0) node[anchor=south] {Start};
		% Stop
		\draw[thin] (106.3-6,-7.8-1.5) rectangle (106.3,-7.8+1.5);
		\foreach \x in {-6,-4.5,...,-1.5}
		{
			\foreach \y in {-1.5,0}
				{
					\filldraw[black, fill=black, thin] (106.3+\x,-7.8+\y) rectangle (106.3+\x+0.75,-7.8+\y+0.75);
					\filldraw[black, fill=black, thin] (106.3+\x+0.75,-7.8+\y+0.75) rectangle (106.3+\x+1.5,-7.8+\y+1.5);
				}
		}
		\draw (106.3-3,-7.8) node[anchor=south] {Stop};
		\end{tikzpicture}
	}
	\caption{Navigational Track.}
	\label{fig:nav-track}
\end{figure*}

The navigational track being used for this work is consists of straight line segments connected at 45$^\circ$ degree angles with the track being approximately 50.4 centimeters in width. The dimensions of the line segments are shown in Fig. \ref{fig:nav-track} for reproduction purposes but the track occurs in a residential home with wood and vinyl tiles in dim to moderate lighting. Each of the models and schemes were used to attempt a single navigation of the track with the default implementation of \cite{bechtel2018} of 50\% throttle with 2 CPUs being used for the CNN inference. Since all models were initially trained on a different dataset, none of the schemes used to successfully navigate the track. The most successful scheme was the B\`ezier image-to-point (1:1 ratio of images to steering wheel angles) which managed to autonomously turn on the 287-45.7-86.4 joining segments. Otherwise both the B\`ezier image-to-point and traditional image-to-point had strong tendencies to drive straight with human intervention required for most of the turns. The least successful model was the B\`ezier image-to-curve model which was only turning right but this is regarded as an issue with the different track environment from training rather than a general performance behavior. For future repeatability, the associated scripts for implementing this work as required for DeepPicar by its previous work \cite{bechtel2018} are included in the associated GitHub repo in the deeppicar-scripts subdirectory. Both the first person view of the DeepPicar driving as well as an exterior view are also given in the GitHub repo in the deeppicar-testing folder of the reports subdirectory.

\section{Results and Conclusions}

This work illustrated that the B\`ezier CNNs can be used to generate smoother and more predictable behavior than traditional CNNs in image-to-point controllers. While the use of B\`ezier output CNNs as image-to-point controllers are consistently better than standard CNN implementations, the image-to-point B\`ezier CNN was only performed better than the standard CNN when analyzing the results of the training and was not experimentally shown. Although reserved the results suggest that B\`ezier CNNs could yield performance increases and output smoothness if integrated into currently developing autonomous vehicles.

\section{Future Work}

The training dataset \cite{bechtel2018} is initially limiting; wheel angles are only three variations and there is a one-to-one correspondence between wheel angles and video frames. Curve fitting to noisy data best works with more sampling points so it would preferable to have a one-to-many mapping between image frames and recorded wheel angles. Rather than constructing the dataset by forming corresponding pairs of images and wheel angles, there could two threads of data collection with one thread for the camera at a nominal operational rate and the second thread is for recording the wheel steering angle at a faster but sustainable sensor rate. Alternatively the original \cite{bechtel2018} could be modified by estimating intermediate point values to upsample the presented data from 20 Hz to an artificially faster sensor rate.

Aside from the model training improvements, the physical platform testing could be improved. Rather than testing in an entirely new environment, it would be better to create another track at the original DeepPicar track location to match the environment. This will help to give a more unbiased assessment of the models and controller schemes behavior.

\bibliographystyle{IEEEtran}
\bibliography{final-report}

\end{document}
