{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-to-Point Model Training\n",
    "\n",
    "This notebook takes the work done in the Inital Exploration notebook and trains a convolutional neural network (CNN) to generate a wheel angle in real dimensional space. The network architecture will follow the [DAVE-2](https://developer.nvidia.com/blog/deep-learning-self-driving-cars/) as previously done by NVIDIA and train using the [DeepPicar-v2](https://github.com/mbechtel2/DeepPicar-v2) dataset and the same model car. The first thing to do is to build the CNN.\n",
    "\n",
    "## Model Building\n",
    "\n",
    "Since this project is not being made into an overall package, what will be done is simply copy the relavent cells from the initial exploration to build the CNN in Tensorflow V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import cv2              # an open computer vision module\n",
    "import glob                      # Regix searching of files\n",
    "import imageio                   # loading images in numpy array\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "%matplotlib notebook\n",
    "import numpy as np               # Array/Tensors\n",
    "import os                        # finding paths\n",
    "import pandas as pd              # Dataframe for viewing\n",
    "import random                    # For shuffling a video list\n",
    "import sys                       # manipulating the path variable\n",
    "import tensorflow as tf          # Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_normalization (LayerNo (None, 66, 200, 3)        79200     \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 31, 98, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 47, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 22, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 20, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 331,419\n",
      "Trainable params: 252,219\n",
      "Non-trainable params: 79,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Image input specifications\n",
    "input_img_height = 66\n",
    "input_img_width = 200\n",
    "input_img_channels = 3\n",
    "\n",
    "\n",
    "def create_dave(input_shape):\n",
    "    # Create tensorflow object\n",
    "    dave_2_model = tf.keras.Sequential()\n",
    "\n",
    "    # First is a normalization layer in DAVE-2 but I don't see it in DeepPicar-v2 model-5conv_3fc\n",
    "    # Attempting to normalize over color\n",
    "    dave_2_model.add(tf.keras.layers.LayerNormalization(\n",
    "        axis=[1, 2, 3], trainable=False,\n",
    "        input_shape=input_shape  # Specify ahead of time for compilation\n",
    "    ))\n",
    "\n",
    "    # Second portion is the sequential 5x5 convolutions of increasing filters at 2x2 strides\n",
    "    for i in range(3):\n",
    "        dave_2_model.add(tf.keras.layers.Conv2D(\n",
    "            filters=(24 + i*12),\n",
    "            kernel_size=(5,5),\n",
    "            strides=(2,2),\n",
    "            padding='valid',\n",
    "            data_format='channels_last',\n",
    "            activation=tf.nn.relu\n",
    "        ))\n",
    "\n",
    "    # Third there is another grouping of Conv2D layers but a 3x3 kernel with no stride\n",
    "    for i in range(2):\n",
    "        dave_2_model.add(tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(3,3),\n",
    "            padding='valid',\n",
    "            data_format='channels_last',\n",
    "            activation=tf.nn.relu\n",
    "        ))\n",
    "\n",
    "    # Fourth and final part is a flattening and then reduction to a single output\n",
    "    dave_2_model.add(tf.keras.layers.Flatten())\n",
    "    for n_neurons in [100, 50, 10, 1]:\n",
    "        dave_2_model.add(tf.keras.layers.Dense(\n",
    "            units=n_neurons,\n",
    "            activation=(tf.nn.tanh if n_neurons == 1 else tf.nn.relu)\n",
    "        ))\n",
    "\n",
    "    # Compiling the model to see the initial structure\n",
    "    dave_2_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=['mse'],\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    \n",
    "    return dave_2_model\n",
    "\n",
    "dave_2_model = create_dave((input_img_height, input_img_width, input_img_channels))\n",
    "dave_2_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The original dataset had only discrete anlges for steering, namely the steering angle $\\theta \\in\\lbrace -30,\\ 0,\\ 30 \\rbrace$ but we wish for a continuous controller so a central moving average will be applied to the edge padding. The total dataset spans 11 videos of 11,000 frames but 1,000 frames is already pushing my laptop memory limits. The current plan to tackle this issue is to queue a video to go through, get the estimated continuous controls, and then batch train a video. As for wether to loop through all videos before any potential repeats or simply randomly train, it isn't clear a priori which is better. For now simply iterating it easiest to at least get a simple model. Setting the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab video file list\n",
    "dpv2_video_files = glob.glob(\"../data/external/DeepPicar-v2-data/*.avi\")\n",
    "random.shuffle(dpv2_video_files)  # Shuffle to randomize for train-test split\n",
    "\n",
    "# Some constants for analyzing videos from DeepPicar-v2\n",
    "N_FRAMES = 1000  # All avi files in DeepPicar-v2 are 1,000 frames\n",
    "ORIG_IMG_HEIGHT = 240\n",
    "ORIG_IMG_WIDTH = 320\n",
    "N_VIDEOS = len(dpv2_video_files)\n",
    "\n",
    "\n",
    "# Define some helper functions\n",
    "def key_from_video_filename(vf : str):\n",
    "    directory_seperated = vf.split('\\\\')  # running on Windows\n",
    "    fileno = int(directory_seperated[-1].split('-')[-1].split('.')[0])\n",
    "    key_filename = 'out-key-%i.csv' % fileno\n",
    "    return '\\\\'.join(directory_seperated[:-1] + [key_filename,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter setup\n",
    "f_size = 5\n",
    "f_array = np.ones((f_size,))/f_size\n",
    "n_pad = int(np.ceil(f_size/2))\n",
    "\n",
    "# Training Constants\n",
    "EPOCHS = 50\n",
    "VAL_SPLIT = 0.2\n",
    "BATCH_SIZE = 256  # Want it to be a power of 4\n",
    "N_IMAGES_PER_TRAINING_SEGMENT = 200\n",
    "N_TRAIN_VIDEOS = 9  # 9/11  for train-val and 2/11 for testing\n",
    "N_IMAGE_AUGS = 2  # Just flipping for now\n",
    "N_LOOPS_THROUGH_VIDEOS = 10  # Looks like 4-5 is sufficient\n",
    "training_history = None\n",
    "\n",
    "# Output scaling\n",
    "y_sf = np.pi/6  # 30 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 0:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 1:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 2:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 3:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 4:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 5:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 6:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 7:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 8:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n",
      "Loop 9:\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-2.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-8.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-5.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-11.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-6.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-4.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-9.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-7.avi\n",
      "\t../data/external/DeepPicar-v2-data\\out-video-3.avi\n"
     ]
    }
   ],
   "source": [
    "# Rerunning this cell will append to training.\n",
    "# Additional epochs: EPOCHS*(N_FRAMES/N_IMAGES_PER_TRAINING_SEGMENT)*N_TRAIN_VIDEOS*N_LOOPS_THROUGH_VIDEOS\n",
    "\n",
    "assert N_FRAMES % N_IMAGES_PER_TRAINING_SEGMENT == 0, \"Need even division of video frames over training\"\n",
    "# Loop over videos\n",
    "for i in range(N_LOOPS_THROUGH_VIDEOS):\n",
    "    print(f\"Loop {i}:\")\n",
    "    # Iterate through the videos for training\n",
    "    for vf in dpv2_video_files[:N_TRAIN_VIDEOS]:\n",
    "        print(f\"\\t{vf}\")\n",
    "        \n",
    "        # Load in csv of correct outputs\n",
    "        y_video = np.loadtxt(\n",
    "            fname=key_from_video_filename(vf),\n",
    "            delimiter=',',\n",
    "            skiprows=1,  # ignore the column titles\n",
    "            usecols=2  # Angle is in the third column\n",
    "        )/y_sf\n",
    "\n",
    "        # Pad, filter, and then reduce\n",
    "        temp = np.pad(y_video, pad_width=n_pad, mode='edge')\n",
    "        del y_video\n",
    "        y_video = np.convolve(temp, f_array, mode='same')[n_pad:-n_pad]\n",
    "        del temp\n",
    "\n",
    "        # Allocate training tensors\n",
    "        x_train = np.empty((N_IMAGE_AUGS*N_IMAGES_PER_TRAINING_SEGMENT, input_img_height, input_img_width, 3))\n",
    "        y_train = np.empty((N_IMAGE_AUGS*N_IMAGES_PER_TRAINING_SEGMENT,))\n",
    "\n",
    "        # Preallocate load tensor\n",
    "        x_load = np.empty((N_IMAGES_PER_TRAINING_SEGMENT, ORIG_IMG_HEIGHT, ORIG_IMG_WIDTH, 3))\n",
    "\n",
    "        # Get video capture going\n",
    "        video_capture = cv2.VideoCapture(vf)\n",
    "\n",
    "        # Iterate through video training batches\n",
    "        for j in range(N_FRAMES // N_IMAGES_PER_TRAINING_SEGMENT):\n",
    "\n",
    "            # Load in the N_IMAGES_PER_TRAINING_SEGMENT\n",
    "            for k in range(N_IMAGES_PER_TRAINING_SEGMENT):\n",
    "                _, image = video_capture.read()\n",
    "                x_load[k] = image\n",
    "                del image\n",
    "\n",
    "            # Reshape from AVI to DAVE-2\n",
    "            x_train[:N_IMAGES_PER_TRAINING_SEGMENT] = tf.image.resize(\n",
    "                images=tf.convert_to_tensor(x_load),\n",
    "                size=[input_img_height, input_img_width]\n",
    "            ).numpy()\n",
    "\n",
    "            # Get corresponding outputs\n",
    "            y_train[:N_IMAGES_PER_TRAINING_SEGMENT] = y_video[\n",
    "                j*N_IMAGES_PER_TRAINING_SEGMENT:(j+1)*N_IMAGES_PER_TRAINING_SEGMENT\n",
    "            ]\n",
    "\n",
    "            # Image Augmentation and output correspondance\n",
    "            # Horizontal flip:\n",
    "            x_train[N_IMAGES_PER_TRAINING_SEGMENT:] = x_train[:N_IMAGES_PER_TRAINING_SEGMENT,:,::-1,:]\n",
    "            y_train[N_IMAGES_PER_TRAINING_SEGMENT:] = -y_train[:N_IMAGES_PER_TRAINING_SEGMENT]\n",
    "\n",
    "            # Train the model\n",
    "            session_hist = dave_2_model.fit(\n",
    "                x=x_train, y=y_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                verbose=0,  # 1,\n",
    "                validation_split=VAL_SPLIT,\n",
    "            )\n",
    "\n",
    "            # Append in training history\n",
    "            sess_hist = np.stack((\n",
    "                    np.array(session_hist.history['loss']),\n",
    "                    np.array(session_hist.history['val_loss'])\n",
    "                ), axis=0).T\n",
    "\n",
    "            if training_history is None:\n",
    "                training_history = sess_hist\n",
    "            else:\n",
    "                temp = np.concatenate((training_history, sess_hist), axis=0)\n",
    "                del training_history\n",
    "                training_history = temp\n",
    "                del temp\n",
    "            del sess_hist\n",
    "\n",
    "        # Some resource freeing\n",
    "        video_capture.release()\n",
    "\n",
    "        # Free up memory\n",
    "        del x_train, y_train, x_load, y_video, video_capture\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Verification\n",
    "\n",
    "This portion will check both the training history to demonstrate progress as well as find the accuracy of a classifier and MSE loss function on the test videos to show models validity.\n",
    "\n",
    "### Training History\n",
    "\n",
    "The overall training history of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e3f0b637e4f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mepoch_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "epoch_vec = np.arange(training_history.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].plot(epoch_vec, training_history[:, 0], color='C0', label=\"Training Loss\")\n",
    "ax[0].plot(epoch_vec, training_history[:, 1], color='C1', label=\"Validation Loss\")\n",
    "\n",
    "ax[0].set_xlim(left=0)\n",
    "ax[0].set_ylim(bottom=0)\n",
    "\n",
    "ax[0].set_xlabel(\"Training Epoch\")\n",
    "ax[0].set_ylabel(\"Mean Square Error\")\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epoch_vec[1:], training_history[1:, 0], color='C0', label=\"Training Loss\")\n",
    "ax[1].plot(epoch_vec[1:], training_history[1:, 1], color='C1', label=\"Validation Loss\")\n",
    "\n",
    "ax[1].set_xlim(left=1)\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "ax[1].set_xlabel(\"Training Epoch\")\n",
    "ax[1].set_ylabel(\"Log Mean Square Error\")\n",
    "\n",
    "ax[1].legend()\n",
    "\n",
    "# Put in dashed and solid lines to indicate new data shifts\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above training history shows ....\n",
    "\n",
    "### Model accuracy\n",
    "\n",
    "Going from continuous to discrete again, now trying to show the accuracy of the model if simply turning at 30$^\\circ$ or going straight. This is to give a more direct comparison with DeepPicar-v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.542113080133e-311\n"
     ]
    }
   ],
   "source": [
    "# Some preallocations\n",
    "N_TEST_VIDEOS = N_VIDEOS - N_TRAIN_VIDEOS\n",
    "\n",
    "y_pred = np.empty((N_FRAMES*N_TEST_VIDEOS,))\n",
    "y_act = np.empty((N_FRAMES*N_TEST_VIDEOS,))\n",
    "\n",
    "print(np.min(y_pred))\n",
    "print(np.max(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 372us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 410us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 371us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 360us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 382us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 406us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 351us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 353us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 411us/sample\n",
      "0.0\n",
      "0.21280215680599213\n",
      "200/200 [==============================] - 0s 356us/sample\n",
      "0.21280215680599213\n",
      "0.21280215680599213\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the videos\n",
    "for i, vf in enumerate(dpv2_video_files[N_TRAIN_VIDEOS:]):\n",
    "    # Read in actual steering angle\n",
    "    y_act[i*N_FRAMES:(i+1)*N_FRAMES] = np.loadtxt(\n",
    "            fname=key_from_video_filename(vf),\n",
    "            delimiter=',',\n",
    "            skiprows=1,  # ignore the column titles\n",
    "            usecols=2  # Angle is in the third column\n",
    "        )/y_sf\n",
    "    \n",
    "    # Preallocate load tensor\n",
    "    image_tensor = np.empty((N_IMAGES_PER_TRAINING_SEGMENT, ORIG_IMG_HEIGHT, ORIG_IMG_WIDTH, 3))\n",
    "\n",
    "    # Get video capture going\n",
    "    video_capture = cv2.VideoCapture(vf)\n",
    "\n",
    "    # Iterate through video by training batches since we know that works\n",
    "    for j in range(N_FRAMES // N_IMAGES_PER_TRAINING_SEGMENT):\n",
    "\n",
    "        # Load in the N_IMAGES_PER_TRAINING_SEGMENT\n",
    "        for k in range(N_IMAGES_PER_TRAINING_SEGMENT):\n",
    "            _, image = video_capture.read()\n",
    "            image_tensor[k] = image\n",
    "            del image\n",
    "        \n",
    "        # Preform prediction\n",
    "        y_pred[\n",
    "            (i*N_FRAMES + j*N_IMAGES_PER_TRAINING_SEGMENT):(i*N_FRAMES + (j+1)*N_IMAGES_PER_TRAINING_SEGMENT)\n",
    "        ] = dave_2_model.predict(\n",
    "            x=tf.image.resize(\n",
    "                images=tf.convert_to_tensor(image_tensor),\n",
    "                size=[input_img_height, input_img_width]\n",
    "            ).numpy(),\n",
    "            verbose=1\n",
    "        )[:,0]/y_sf\n",
    "        \n",
    "        print(np.min(y_pred))\n",
    "        print(np.max(y_pred))\n",
    "    \n",
    "    # Some resource freeing\n",
    "    video_capture.release()\n",
    "    del image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do rounding to ensure -1, 0, 1\n",
    "y_pred = np.round(y_pred).astype('int')\n",
    "y_act = np.round(y_act).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. 176.   0.]\n",
      " [  0. 971.   0.]\n",
      " [  0. 853.   0.]]\n",
      "Model accuracy: 48.55%'\n"
     ]
    }
   ],
   "source": [
    "cross_correlation_matrix = np.zeros((3,3))  # correlation matrix to see matches\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        cross_correlation_matrix[i, j] = np.count_nonzero(np.logical_and(\n",
    "            np.equal(y_act, i-1),\n",
    "            np.equal(y_pred, j-1),\n",
    "        ))\n",
    "        \n",
    "\"\"\"\n",
    "Quick note:\n",
    "* column is predicted angle from -30 to 30 in left to right.\n",
    "* row is actual from -30 to 30 from top to bottom.\n",
    "\"\"\"\n",
    "print(cross_correlation_matrix)\n",
    "\n",
    "# Printing out the model statistics of discrete controller in the augmented domain\n",
    "n_total = y_pred.shape[0]\n",
    "n_correct = np.trace(cross_correlation_matrix)\n",
    "n_incorrect = n_total - n_correct\n",
    "print(\"Model accuracy: %2.2f%%'\" % (100*n_correct/n_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief comment about the accuracy is that going straight will be correct ~50% of the time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
